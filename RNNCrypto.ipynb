{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/I3ryI3e/fantastic-octo-barnacle/blob/master/RNNCrypto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "qC3StY1OfX74",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import requests        # for making http requests to binance\n",
        "import json            # for parsing what binance sends back to us\n",
        "import pandas as pd    # for storing and manipulating the data we get back\n",
        "import numpy as np     # numerical python\n",
        "from sklearn import preprocessing  #helps in the preprocessing function\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt # for charts and such\n",
        "import random  \n",
        "import datetime as dt  # for dealing with times\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, CuDNNLSTM, BatchNormalization\n",
        "import os\n",
        "\n",
        "#Fetch data from how many days ago?\n",
        "DAYS_AGO = 4\n",
        "#1 Day = 86400000 miliseconds\n",
        "DAY_TO_MILISECONDS = 86400000\n",
        "#Days to miliseconds\n",
        "DAYS_IN_MILISECONDS = DAYS_AGO * DAY_TO_MILISECONDS\n",
        "#Number of samples (Max 1000)\n",
        "LIMIT = 1000\n",
        "#1 Minute = 60000 mili\n",
        "MIN_EQUALS_MILS= 60000\n",
        "# SEQ_LEN = How much hours are we going to give to the NN so it can predict \n",
        "SEQ_LEN = 60\n",
        "# What Pair are we going to predict\n",
        "RATIO_TO_PREDICT = 'ETHUSDT'\n",
        "#How many hours ahead are we going to predict(if '1' we are going to predict if the price is going to be higher in the next hour)\n",
        "FUTURE_PERIOD_PREDICT = 3\n",
        "#Validation sample split\n",
        "VALIDATION_PERCENTAGE = 0.05\n",
        "TRAINING_DATA = 1-VALIDATION_PERCENTAGE\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 64\n",
        "#Name for the models\n",
        "NAME = f\"{SEQ_LEN}-SEQ--{FUTURE_PERIOD_PREDICT}--PRED--{RATIO_TO_PREDICT}--RATIO--{int(time.time())}\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wCGf-8_jfaeF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Connect to binance API and get the data in each pair we want information on\n",
        "# INPUTS: symbol = The pair(ex= LTC-USDT) // Interval = 1h(1 hour intervals)\n",
        "# OUTPUTS: DataFrame with the data \n",
        "\n",
        "def get_bars(symbol, startTime ,interval = '1m'):\n",
        " root_url = 'https://api.binance.com/api/v1/klines'\n",
        " url = root_url + '?symbol=' + symbol + '&interval=' + interval + '&startTime=' + str(startTime) + '&limit=' + str(LIMIT) \n",
        " data = json.loads(requests.get(url).text)\n",
        " df = pd.DataFrame(data)\n",
        " df.columns = ['open_time',\n",
        "               'open_price', 'high', 'low', 'close_price', 'volume',\n",
        "               'close_time', 'qav', 'num_trades',\n",
        "               'taker_base_vol', 'taker_quote_vol', 'ignore']\n",
        " df=df.drop(columns=['close_time', 'qav', 'num_trades',\n",
        "               'taker_base_vol', 'taker_quote_vol', 'ignore'])\n",
        " return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WgFwo8ntfeQK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Does the targets for the network.\n",
        "#INPUTS: Current price; Future Price\n",
        "#OUTPUT: 1 if the Future Price is higher than the current price, 0 otherwise\n",
        "\n",
        "def classify(current, future):\n",
        "  if float(future) > float(current):\n",
        "    return 1\n",
        "  else:\n",
        "    return 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "agOWJSA39DRf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess(df):\n",
        "  df = df.drop('future', 1)\n",
        "  pd.set_option('use_inf_as_na', True)\n",
        "  for col in df.columns:\n",
        "    if col != 'target':\n",
        "      df[col]=df[col].pct_change()\n",
        "      df.dropna(inplace=True)\n",
        "      df[col] = preprocessing.scale(df[col].values)\n",
        "      \n",
        "  df.dropna(inplace=True)\n",
        "  sequential_data = []\n",
        "  prev_days= deque(maxlen=SEQ_LEN)\n",
        "  #Making the sequeces // appending values until we get to SEQ_LEN and add it to sequential_data\n",
        "  #that's going to be a list of sequences\n",
        "  for i in df.values:\n",
        "      prev_days.append([n for n in i[:-1]])\n",
        "      if len(prev_days) == SEQ_LEN:\n",
        "        sequential_data.append([np.array(prev_days),i[-1]])\n",
        "  random.shuffle(sequential_data)\n",
        "  \n",
        "  buys = []\n",
        "  sells = []\n",
        "  \n",
        "  for seq, target in sequential_data:\n",
        "    if target == 0:\n",
        "      sells.append([seq,target])\n",
        "    elif target == 1:\n",
        "      buys.append([seq,target])\n",
        "      \n",
        "  random.shuffle(buys)\n",
        "  random.shuffle(sells)\n",
        "   \n",
        "  lower = min(len(buys), len(sells))\n",
        "  \n",
        "  buys = buys[:lower]\n",
        "  sells = sells[:lower]\n",
        "  \n",
        "  sequential_data = buys+sells\n",
        "  \n",
        "  random.shuffle(sequential_data)\n",
        "  \n",
        "  X = []\n",
        "  Y = []\n",
        "  \n",
        "  for seq, target in sequential_data:\n",
        "    X.append(seq)\n",
        "    Y.append(target)\n",
        "  \n",
        "  return np.array(X),Y\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XJVBSeKKeXfC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Preparing all the DataFrame to work with\n",
        "\n",
        "root_url = 'https://api.binance.com/api/v1/time'\n",
        "now = json.loads(requests.get(root_url).text)\n",
        "now = now.get('serverTime')\n",
        "earlier5days = now - DAYS_IN_MILISECONDS\n",
        "btcusdt = pd.DataFrame()\n",
        "ethusdt = pd.DataFrame()\n",
        "ltcusdt = pd.DataFrame()\n",
        "xrpusdt = pd.DataFrame()\n",
        "\n",
        "for x in range(earlier5days, now, (LIMIT*MIN_EQUALS_MILS)):\n",
        "  btcusdt = pd.concat([btcusdt,get_bars('BTCUSDT',x)])\n",
        "  ethusdt = pd.concat([ethusdt,get_bars('ETHUSDT',x)])\n",
        "  ltcusdt = pd.concat([ltcusdt,get_bars('LTCUSDT',x)])\n",
        "  xrpusdt = pd.concat([xrpusdt,get_bars('XRPUSDT',x)])\n",
        "\n",
        "main_df = pd.DataFrame(ethusdt)\n",
        "main_df= pd.merge(main_df,btcusdt,on='open_time',how='left',suffixes=('_ETHUSDT','_BTCUSDT')).fillna(method='ffill')\n",
        "\n",
        "ltcusdtColumNames=ltcusdt.columns.tolist()\n",
        "renamedNamesLTC=[]\n",
        "for name in ltcusdtColumNames:\n",
        "  renamedNamesLTC.append(name+\"_LTCUSDT\")\n",
        "      \n",
        "renamedNamesLTC=dict(zip(ltcusdtColumNames, renamedNamesLTC))\n",
        "ltcusdt=ltcusdt.rename(index=str, columns=renamedNamesLTC)\n",
        "\n",
        "xrpusdtColumNames=xrpusdt.columns.tolist()\n",
        "renamedNamesXRP=[]\n",
        "for name in xrpusdtColumNames:\n",
        "  renamedNamesXRP.append(name+\"_XRPUSDT\")\n",
        "      \n",
        "renamedNamesXRP=dict(zip(xrpusdtColumNames, renamedNamesXRP))\n",
        "xrpusdt=xrpusdt.rename(index=str, columns=renamedNamesXRP)\n",
        "\n",
        "\n",
        "main_df= pd.concat([main_df.reset_index(drop=True),ltcusdt.reset_index(drop=True)], axis=1)\n",
        "main_df= pd.concat([main_df.reset_index(drop=True),xrpusdt.reset_index(drop=True)], axis=1)\n",
        "main_df.index = [dt.datetime.fromtimestamp(x/1000.0) for x in main_df.open_time]\n",
        "main_df=main_df.drop(columns=['open_time','open_time_LTCUSDT','open_time_XRPUSDT'])\n",
        "\n",
        "\n",
        "main_df['future'] = main_df[f'close_price_{RATIO_TO_PREDICT}'].shift(-FUTURE_PERIOD_PREDICT)\n",
        "main_df['target'] = list(map(classify, main_df[f'close_price_{RATIO_TO_PREDICT}'], main_df['future']))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "30wyFx2vsRcM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Separate Validation data -- Last VALIDATION_PERCENTAGE of the data\n",
        "\n",
        "times = sorted(main_df.index.values)\n",
        "last_Xpct = times[-int(VALIDATION_PERCENTAGE*len(times))]\n",
        "\n",
        "validation_data = main_df[(main_df.index >= last_Xpct)]\n",
        "main_df= main_df[(main_df.index < last_Xpct)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-VF8vLl2zjTm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "outputId": "38b086ae-7144-45e5-9679-1e2dc999ff75"
      },
      "cell_type": "code",
      "source": [
        "for col in main_df.columns:\n",
        "  main_df[col]= pd.to_numeric(main_df[col])\n",
        "for col in validation_data.columns:\n",
        "  validation_data[col] = pd.to_numeric(validation_data[col])\n",
        "  \n",
        "train_x,train_y = preprocess(main_df)\n",
        "validation_x, validation_y = preprocess(validation_data)\n",
        "\n",
        "print(f\"train data: {len(train_x)} validation: {len(validation_x)}\")\n",
        "print(f\"Dont buys: {train_y.count(0)}, buys: {train_y.count(1)}\")\n",
        "print(f\"VALIDATION Dont buys: {validation_y.count(0)}, buys: {validation_y.count(1)}\")\n",
        "\n",
        "model = Sequential()\n",
        "model.add(CuDNNLSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())  #normalizes activation outputs, same reason you want to normalize your input data.\n",
        "\n",
        "model.add(CuDNNLSTM(128, return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(CuDNNLSTM(128))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=opt,\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "if not os.path.exists('logs'):\n",
        "    os.makedirs('logs')\n",
        "  \n",
        "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
        "\n",
        "if not os.path.exists('models'):\n",
        "    os.makedirs('models')\n",
        "\n",
        "filepath = \"RNN_Final-{epoch:02d}-{val_acc:.3f}\"  # unique file name that will include the epoch and the validation acc for that epoch\n",
        "checkpoint = ModelCheckpoint(\"models/{}.model\".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')) # saves only the best ones\n",
        "\n",
        "history = model.fit(\n",
        "    train_x, train_y,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(validation_x, validation_y),\n",
        "    callbacks=[tensorboard, checkpoint],\n",
        ")\n",
        "\n",
        "# Score model\n",
        "score = model.evaluate(validation_x, validation_y, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "# Save model\n",
        "model.save(\"models/{}\".format(NAME))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train data: 4998 validation: 198\n",
            "Dont buys: 2499, buys: 2499\n",
            "VALIDATION Dont buys: 99, buys: 99\n",
            "Train on 4998 samples, validate on 198 samples\n",
            "Epoch 1/10\n",
            "4998/4998 [==============================] - 64s 13ms/step - loss: 0.7850 - acc: 0.5200 - val_loss: 0.6912 - val_acc: 0.5000\n",
            "Epoch 2/10\n",
            "4998/4998 [==============================] - 60s 12ms/step - loss: 0.7175 - acc: 0.5416 - val_loss: 0.6963 - val_acc: 0.5152\n",
            "Epoch 3/10\n",
            "4998/4998 [==============================] - 59s 12ms/step - loss: 0.7109 - acc: 0.5440 - val_loss: 0.6933 - val_acc: 0.5455\n",
            "Epoch 4/10\n",
            "4998/4998 [==============================] - 59s 12ms/step - loss: 0.6830 - acc: 0.5712 - val_loss: 0.7197 - val_acc: 0.4646\n",
            "Epoch 5/10\n",
            "4998/4998 [==============================] - 60s 12ms/step - loss: 0.6731 - acc: 0.5908 - val_loss: 0.7483 - val_acc: 0.4293\n",
            "Epoch 6/10\n",
            "4998/4998 [==============================] - 59s 12ms/step - loss: 0.6691 - acc: 0.5908 - val_loss: 0.7447 - val_acc: 0.4949\n",
            "Epoch 7/10\n",
            "4998/4998 [==============================] - 60s 12ms/step - loss: 0.6548 - acc: 0.6066 - val_loss: 0.7398 - val_acc: 0.4444\n",
            "Epoch 8/10\n",
            "4998/4998 [==============================] - 59s 12ms/step - loss: 0.6341 - acc: 0.6379 - val_loss: 0.7753 - val_acc: 0.4495\n",
            "Epoch 9/10\n",
            "4998/4998 [==============================] - 59s 12ms/step - loss: 0.6256 - acc: 0.6445 - val_loss: 0.7219 - val_acc: 0.5051\n",
            "Epoch 10/10\n",
            "4998/4998 [==============================] - 59s 12ms/step - loss: 0.6065 - acc: 0.6677 - val_loss: 0.8010 - val_acc: 0.4495\n",
            "Test loss: 0.800954980681641\n",
            "Test accuracy: 0.44949494979598303\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}